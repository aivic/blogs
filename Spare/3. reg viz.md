## Introduction
In this guide you are going to learn about the fundamentals of plotting regression and its variants along with their derivative features using the Seaborn library.

By the end of this guide you will be able to implement the following concepts:
1. Visualizing a linear regression
2. Visualizing a polynomial and logistic regression 
3. Handling the plot aesthetics

## The Baseline
In this guide we are going to use the following libraries:

**Syntax**


```python
# Importing necessary libraries
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
```

## Visualizing a Linear Regression
As per the Merriam-Webster dictionary, a linear regression is:

> the process of finding a straight line (as by least squares) that best approximates a set of points on a graph.

A simple linear regression has one dependent (target) variable and one independent variable. Using the independent variable, we have to estimate the future values of the dependent variable. 

To implement any of the regression variants, you can use the `Scikit-Learn` library. Here, let us try to find a best fit line between a dependent and an independent variable.

**Generating data for simple linear regression**


```python
# Initializing the data
X = np.array([1, 5, 10, 13, 16, 20, 23, 26, 29, 20])
y = np.array([1, 6, 6, 10, 5, 9, 8, 11, 7, 25])

# Storing the data in a DataFrame
sl = pd.DataFrame({'Independent': X, 'Dependent': y})
```

**Visualizing the best fit line along with the confidence interval**
The best fit line is plotted using the `lmplot` method available in the Seaborn.

```python
# Plotting
with sns.color_palette('summer'):
    sns.lmplot(x="Independent", y="Dependent", data=sl)

# Labelling the title, x-label and y-label
plt.title('Simple Linear Regression Plot', weight='bold', fontsize=18)
plt.xlabel('Independent', fontsize=16)
plt.ylabel('Dependent', fontsize=16)

# Displaying the plot
plt.show()
```

![SLR](https://i.imgur.com/KsjszlE.png)

From the above graph, following observations can be made:
1. There is a large confidence interval area around the best fit line, shown in shaded area.
2. There is an outlier at coordinate (20, 25) which has affected the position of the line as well as the confidence interval.

**Verification of simple linear regression**
We are able to plot the best fit line on the given dataset and also we figured out that one data point has affected the result, however, if it was a large dataset then arriving at such conclusion could have become bit more difficult. Therefore, in such cases it's advised to use residual plots.

If the values around the `y=0` in a residual plot (implemented by the `residplot` method in the Seaborn) are scattered randomly then the linear regression model is a good one. However, if there is any pattern in the residuals plotted then the model is a bad one. More stronger the pattern, much worse the model is.

Let us build the residual plot on the above dataset:


```python
# Setting the figure size
plt.figure(figsize=(8, 5))

# Plotting
with sns.color_palette('summer'):
    sns.residplot(x="Independent", y="Dependent", data=sl)

# Labelling the title, x-label and y-label
plt.title('Residual Plot', weight='bold', fontsize=18)
plt.xlabel('Fitted values', fontsize=16)
plt.ylabel('Residuals', fontsize=16)

# Displaying the plot
plt.show()
```

![Resid](https://i.imgur.com/cLkPgsT.png)

As you can observe there is no pattern across `y=0`. However, the outlier data point is separated from the other points which if not considered can improve the model.

**Visualizing the best fit line along without the outlier**
First, we are going to remove the outlier from the data. To do that we can use the following code:


```python
# Dropping the last record consisting of outlier
sl = sl.drop(9)
```

Next, we will plot the simple linear regression along with its residual plot.

```python
# You can further use `robust=True` argument inside the `lmplot` method.
# This will further reduce the affect of outliers on the model.

# Plotting
with sns.color_palette('summer'):
    sns.lmplot(x="Independent", y="Dependent", data=sl)

# Labelling the title, x-label and y-label
plt.title('Simple Linear Regression Plot', weight='bold', fontsize=18)
plt.xlabel('Independent', fontsize=16)
plt.ylabel('Dependent', fontsize=16)

# Displaying the plot
plt.show()
```

![SLR_NO](https://i.imgur.com/zy8Gc4P.png)


```python
# Plotting
with sns.color_palette('summer'):
    sns.residplot(x="Independent", y="Dependent", data=sl)

# Labelling the title, x-label and y-label
plt.title('Residual Plot', weight='bold', fontsize=18)
plt.xlabel('Fitted values', fontsize=16)
plt.ylabel('Residuals', fontsize=16)

# Displaying the plot
plt.show()
```

![Resid](https://i.imgur.com/5TpUyP0.png)

## Visualizing a Polynomial and Logistic Regression 
