## Introduction

In this guide you will learn about different types of classification algorithm present in scikit-learn.

## Scikit-learn 

Scikit-learn is a famous open source Python library that implements a range of operations in machine learning i.e pre-processing, cross-validation and visualization algorithms using a unified interface.

To install Scikit-learn library ,simply type the following command in your command prompt.

**pip install scikit-learn
**

Unique features of scikit-learn are :

1. It has simple tool for data mining and data analysis. It can be used in various classification, regression and clustering    algorithms like support vector machines, random forests, gradient boosting, k-means, etc.
2. It is open source , hence any one can access it and can customize it .
3. Built on the top of libraries like NumPy, SciPy, and matplotlib.

Consider the case where students has written a exam for three subjects and the the results has been announced .And the jumior class students wants to predict their respective results using the data collected by the senior students.  A preview of first few rows of the file is shown below:

The above data has been read directly in a Pandas DataFrame. So, let us learn to implement classifiers present in scikit-learn
 keeping the following objectives in mind:
 
1. There are three columns , which which reprsent the marks of students secured in three subjects.
2. Remove the first three rows which consist of garbage data.
3. The last column rows holds the record whether the student has passed the exam or not. 
4. Here divide the dependend column i.e whether the student has passed or failed , put this in column Y and seperate from the dataset . And independent variable i.e marks , put this in X column . 

## The baseline

To initiate the I/O process, the first step is to import the **numpy** and **sklearn**  library.


```python
import numpy as np
from sklearn import tree
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
```

In the next step , initiate the train data used for classification.


```python
X = [[18, 80, 44], [17, 70, 43], [16, 60, 38], [15, 54, 37], [16, 65, 40],
    [19, 90, 47], [17, 64, 39],
    [17, 70, 40], [15, 55, 37], [17, 75, 42], [18, 85, 43], [18, 75, 41], [18, 77, 41]]

Y = ['pass', 'pass', 'fail', 'fail', 'pass', 'pass', 'fail', 'fail',
    'fail', 'pass', 'pass', 'fail', 'fail']

```

Here X  represents independend variables , where the columns are not dependent on anything. And Y represents the dependend variable , which is dependend on X i.e independend varaible.

In the next step we will initialize the test data . In testing data , we test the classification models aganist it.


```python
test_data = [[19, 70, 43],[14, 75, 42],[18,65,40]]
test_labels = ['pass','pass','pass']
```

## Classification 

In supervised machine learning , there is **Regression** and **Classification** .

Classification mainly deals with classifying the datasets , where the output will be like 'yes or no' , 'pass or fail ' kind of predictions.

We are going to use four classifiers provided by scikit-learn. They are:
1. Decision tree.
2. Random forest.
3. Logistic regression.
4. Support Vector Classifier (SVC).


## Decision tree.

**Decision trees** are supervised learning algorithms. Decision trees are assigned to the information based learning algorithms which use different measures of information gain for learning. You  can use decision trees for issues where You have continuous but also categorical input and target features. 

The main job of decision trees is to find those illustrative features which contain the most "information" regarding the target feature and then split the dataset along the values of these features such that the target feature values for the resulting sub_datasets are as pure as possible .

The advantage of Decision Tree Classifier is, it is  a simple and widely used classification technique. It has a straitforward approach to solve the classification problem. 

Building a optimal decision tree is significant in decision tree classifier. In general, may decision trees can be constructed from a given set of attributes. While some of the trees are more accurate than others, finding the efficient tree is computationally infeasible because of the large size of the search space.

There are many  efficent algorithms which have been developed to construct a far more accurate decision tree. These algorithms ususally employ a greedy approach  that grows a decision tree by making a series of locally ideal decisions about which attribute to be used for partitioning the data. For example, ID3, C4.5, CART are greedy decision tree induction algorithms.


```python
#DecisionTreeClassifier
decision_tree = tree.DecisionTreeClassifier()
decision_tree = decision_tree.fit(X,Y)
dtc_prediction = decision_tree.predict(test_data)
print (dtc_prediction)
```

## Random forest.

**Random forest** is also a supervised learning algorithm.  It is also the most feasible  and easy to use algorithm. As you know a forest is comprised of trees , it is same in this case also . Random forests creates decision trees on randomly selected data samples, gets prediction from each tree and selects the best solution by means of voting. We can use Random forest for knowing the feature importance of each feature.

Random forest is a machine learning algorithm which uses ensemble method. It is said that, a random forest is made up of numerous decision trees and helps to tackle the problem of overfitting in decision trees. These decision trees are randomly constructed by selecting random features from the given dataset.

Random forest is calculated on the basis of the maximum number of votes received from the decision trees. The outcome which is arrived at th end is , a maximum number of times through the numerous decision trees is considered as the final outcome by the random forest.


```python
#RandomForestClassifier
random_forest = RandomForestClassifier()
random_forest.fit(X,Y)
rfc_prediction = random_forest.predict(test_data)
print (rfc_prediction)
```

## Support Vector Classifier (SVC)

**Support Vector Machine (SVM)** is a supervised machine learning algorithm which can be used for both classification or regression challenges. However,  it is mostly used in classification problems. In this algorithm, we plot each data item as a point in n-dimensional space (where n is number of features you have) with the value of each feature being the value of a particular coordinate. Then, we perform classification by finding the hyper-plane that differentiate the two classes very well.

In Support Vector Classifier, it is beneficial to have a linear hyper-plane between  two classes. But, there is a question which comes, should you add this feature manually to have a hyper-plane. SVC uses a trick called the kernel trick. kernel trick are functions which takes low dimensional input space and transform it to a higher dimensional space. It is mostly useful in non-linear separation problem. Simply put, it does some complex data transformations, to find out the way to process to the process to separate the data based on the outputs you have defined.


```python
#Support Vector Classifier
support_vector = SVC()
support_vector.fit(X,Y)
s_prediction = support_vector.predict(test_data)
print (s_prediction)

```

## Logistic regression.

**Logistic regression** is one of the most famous machine learning algorithm along with linear regression. In many ways, linear regression and logistic regression are similar. But, the biggest difference is  Linear regression algorithms are used to predict/forecast values but logistic regression is used for classification tasks.

The output of Logistic Regression is a sigmoid curve or S-curve. Where the value on the independent variable would determine the dependent variable .As said earlier,in logistic regression there are only two possible outcomes i.e 0 and 1. Logistic Regression use a threshold value to make  prediction easier. If the independent variable corresponding dependent variable or probability),is lesser than the threshold value, the outcome is taken as 0 and if it is greater than the value, the outcome is taken as 1.


```python
#LogisticRegression
logistic = LogisticRegression()
logistic.fit(X,Y)
l_prediction = logistic.predict(test_data)
print (l_prediction)
```

After knowing the Classification algorithms , lets see how to evaluate the algorithms.

In Machine learning, fitting the model is not important but to find the correct model is very important . So there is something called **Error metrics**. Error metrics define how  well the testing data is fitting the model. 

There are many error Metrics for Classification.The most important model evaluation error metric is  **Accuracy**.

To know the accuracy  there is simple way. This is can be done using the** accuracy()**.

Here the accuracy score of Decision Tree Classifier is shown. 


```python
from sklearn.metrics import accuracy_score
DecisionTreeClassifier = accuracy_score(dtc_prediction,test_labels)
print(DecisionTreeClassifier)
```

    0.6666666666666666
    

0.6666666666666666 is the Accuracy score for Decision Tree Classifier.

## Conclusion

By going through this guide, you have covered the Sklearn classifiers and how to measure their accuracy.


