## Introduction
In the previous [guide](), you were introduced to the SciPy stats module to perform statistical analysis in Python. In this guide, you will learn about various optimization algorithms available in SciPy. 

By the end of this guide you'll be able to learn the following topics:

1. Minimize method
2. Global optimization 
3. Least-squares minimization 
4. Root finding

## The Baseline
Throughout this guide, we will be using the following libraries:


```python
import numpy as np
import scipy as sp
from scipy import optimize
```

## Minimize Method
The minimize function provides a common interface to unconstrained and constrained minimization algorithms for multivariate scalar functions in scipy.optimize. Let us take the Rosenbrock function to demonstrate the minimization function on N variables.


```python
# ============================
# Minimizing the Rosenbrock Function
# ============================
def rosenbrock(x):    
    return sum(100.0*(x[1:]-x[:-1]**2.0)**2.0 + (1-x[:-1])**2.0)

x0 = np.array([1.3, 0.7, 1.8, 2.9, 1.8])
res = optimize.minimize(rosenbrock, x0, method='nelder-mead',
               options={'xtol': 1e-8, 'disp': True})

# Optimization terminated successfully.
#          Current function value: 0.000000
#          Iterations: 527
#          Function evaluations: 867

res.x

# array([1.        , 1.        , 1.        , 1.        , 0.99999999])
```

