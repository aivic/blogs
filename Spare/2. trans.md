## Introduction
In the previous [guide](), you were introduced to the SciPy stats module to perform statistical analysis in Python. In this guide, you will learn about various optimization algorithms available in SciPy. 

By the end of this guide you'll be able to learn the following topics:

1. Minimize method
2. Global optimization 
3. Least-squares minimization 
4. Root finding

## The Baseline
Throughout this guide, we will be using the following libraries:


```python
import numpy as np
import scipy as sp
from scipy import optimize
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
```

## Minimize Method
The minimize function provides a common interface to unconstrained and constrained minimization algorithms for multivariate scalar functions in scipy.optimize. Let us take the Rosenbrock function to demonstrate the minimization function on N variables.


```python
# ============================
# Minimizing the Rosenbrock Function
# ============================
def rosenbrock(x):    
    return sum(100.0*(x[1:]-x[:-1]**2.0)**2.0 + (1-x[:-1])**2.0)

x0 = np.array([1.3, 0.7, 1.8, 2.9, 1.8])
res = optimize.minimize(rosenbrock, x0, method='nelder-mead',
               options={'xtol': 1e-8, 'disp': True})

# Optimization terminated successfully.
#          Current function value: 0.000000
#          Iterations: 527
#          Function evaluations: 867

res.x

# array([1.        , 1.        , 1.        , 1.        , 0.99999999])
```

The current method doesn't use the concept of gradient evaluation and hence it may take longer sometimes to find the minimum.  

In order to converge quickly with less number of iterations, a gradient evaluation routine can be set up as shown in the code below:


```python
# ============================
# Function to compute gradient
# ============================
def rosen_grad(x):
    xm = x[1:-1]
    xm_m1 = x[:-2]
    xm_p1 = x[2:]
    der = np.zeros_like(x)
    der[1:-1] = 200*(xm-xm_m1**2) - 400*(xm_p1 - xm**2)*xm - 2*(1-xm)
    der[0] = -400*x[0]*(x[1]-x[0]**2) - 2*(1-x[0])
    der[-1] = 200*(x[-1]-x[-2]**2)
    return der

# The gradient information is specified in the minimize
# method and passed to the jac argument as shown below

res = optimize.minimize(rosenbrock, x0, method='BFGS', jac=rosen_grad,
               options={'disp': True})

# Optimization terminated successfully.
#          Current function value: 0.000000
#          Iterations: 40
#          Function evaluations: 47
#          Gradient evaluations: 47

res.x
# array([1.        , 1.00000002, 1.00000004, 1.00000007, 1.00000014])
```

## Global Optimization
Global optimization aims to find the global minimum of a function within given bounds, in the presence of potentially many local minima. Typically global minimizers efficiently search the parameter space, while using a local minimizer (e.g. minimize) under the hood.

Let us create a data to implement the global optimization:


```python
# Creating data with many local and global minima
def peaks(x):
    return (-(x[1] + 47) * np.sin(np.sqrt(abs(x[0]/2 + (x[1]  + 47))))
            -x[0] * np.sin(np.sqrt(abs(x[0] - (x[1]  + 47)))))

bounds = [(-512, 512), (-512, 512)]

# Visualizing the data
x = np.arange(-512, 513)
y = np.arange(-512, 513)
xgrid, ygrid = np.meshgrid(x, y)
xy = np.stack([xgrid, ygrid])

fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')
ax.view_init(45, -45)
ax.plot_surface(xgrid, ygrid, peaks(xy), cmap='terrain')
ax.set_xlabel('x')
ax.set_ylabel('y')
ax.set_zlabel('Peaks(x, y)')
plt.show()
```

![Peaks](https://i.imgur.com/rjv9ma3.png)

We now use the global optimizers to obtain the minimum and the function value at the minimum. 


```python
# Storing results in a dictionary
results = dict()

# Various Global Optimizers
results['SHGO'] = optimize.shgo(peaks, bounds)
results['SHGO_SOBOL'] = optimize.shgo(peaks, bounds, n=200, iters=5, sampling_method='sobol')
results['DA'] = optimize.dual_annealing(peaks, bounds)
results['DE'] = optimize.differential_evolution(peaks, bounds)
results['BH'] = optimize.basinhopping(peaks, bounds)

# Visualizing the minima for each optimizer
fig = plt.figure()
ax = fig.add_subplot(111)
im = ax.imshow(peaks(xy), interpolation='bilinear', origin='lower',
               cmap='gray')
ax.set_xlabel('x')
ax.set_ylabel('y')

def plot_point(res, marker='o', color=None):
    ax.plot(512+res.x[0], 512+res.x[1], marker=marker, color=color, ms=10)

plot_point(results['BH'], color='y')  # basinhopping           - yellow
plot_point(results['DE'], color='c')  # differential_evolution - cyan
plot_point(results['DA'], color='w')  # dual_annealing.        - white

# SHGO produces multiple minima, plot them all (with a smaller marker size)
plot_point(results['SHGO'], color='r', marker='+')
plot_point(results['SHGO_SOBOL'], color='r', marker='x')
for i in range(results['SHGO_SOBOL'].xl.shape[0]):
    ax.plot(512 + results['SHGO_SOBOL'].xl[i, 0],
            512 + results['SHGO_SOBOL'].xl[i, 1],
            'ro', ms=2)

ax.set_xlim([-4, 514*2])
ax.set_ylim([-4, 514*2])
plt.show()
```

![min](https://i.imgur.com/9T1xjNq.png)

